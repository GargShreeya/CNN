{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        self.weights=np.random.normal(0, 0.1,( input_size, output_size))\n",
    "        self.bias=0\n",
    "        self.in_size=input_size\n",
    "        self.out_size=output_size\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        self.input=input_x\n",
    "        output=np.dot(input_x, self.weights) + self.bias\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_loss, lr):\n",
    "        grad_W=np.dot(self.input.T, grad_loss)\n",
    "        grad_Bias=np.sum(grad_loss)\n",
    "        self.weights-=lr*grad_W/self.input.shape[0]\n",
    "        self.bias-=lr*grad_Bias/ self.input.shape[0]\n",
    "        return (np.dot(grad_loss,self.weights.T))\n",
    "\n",
    "    def backwardL2reg(self, grad_loss, lr):\n",
    "        grad_reg=np.dot(self.weight, grad_loss)\n",
    "        grad_W=np.dot(self.input.T, grad_loss) + grad_reg\n",
    "        grad_Bias=np.sum(grad_loss)\n",
    "        self.weights-=lr*grad_W/self.input.shape[0]\n",
    "        self.bias-=lr*grad_Bias/ self.input.shape[0]\n",
    "        return (np.dot(grad_loss,self.weights.T))\n",
    "\n",
    "    def backwardL1reg(self, grad_loss, lr):\n",
    "        grad_reg=grad_loss\n",
    "        grad_W=np.dot(self.input.T, grad_loss) + grad_reg\n",
    "        grad_Bias=np.sum(grad_loss)\n",
    "        self.weights-=lr*grad_W/self.input.shape[0]\n",
    "        self.bias-=lr*grad_Bias/ self.input.shape[0]\n",
    "        return (np.dot(grad_loss,self.weights.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, in_channels,filter_size,filter_num, padding=0, stride=1, bias=True,  conv_layer=1):\n",
    "        sigma=0.1\n",
    "        self.filter_size=filter_size\n",
    "        self.filter=filter_num\n",
    "        self.padding=padding\n",
    "        self.stride=stride\n",
    "        self.in_channels=in_channels\n",
    "        self.weights=np.random.normal(0, sigma, size=(filter_num,in_channels,filter_size, filter_size))\n",
    "        # self.weights=np.random.randn(filter_num,in_channels,filter_size, filter_size)\n",
    "        self.bias=np.zeros(self.filter)\n",
    "        self.conv_layer=conv_layer\n",
    "        \n",
    "    def forward(self,input):\n",
    "        batch_size, in_channels, height, width=input.shape\n",
    "        #Input shape= (Batch_size, in_channels, in_height, in_width)\n",
    "        pad_size=self.padding\n",
    "        #adding padd to the input\n",
    "        padded_input=np.zeros(shape=(batch_size, in_channels, height+2*pad_size, width+2*pad_size), dtype='int32')\n",
    "        padded_input[:,:, pad_size:height+pad_size, pad_size: width+pad_size]=input\n",
    "        # Reminder: Save Input for backward-prop\n",
    "        self.input=padded_input\n",
    "        # Simple Conv operation:\n",
    "        self.out_height=((height-self.filter_size + 2*pad_size)//self.stride)+1\n",
    "        self.out_width=((width-self.filter_size + 2*pad_size)//self.stride)+1\n",
    "        self.out_channel=self.filter\n",
    "        output=np.zeros(shape=(batch_size, self.out_channel,self.out_height, self.out_width ), dtype='float64')\n",
    "        # Loop over every location in inp_height * inp_width for the whole batch\n",
    "        offset=self.filter_size//2\n",
    "        for depth in range(self.out_channel):\n",
    "            for batch in range(batch_size):\n",
    "                for m in range(offset,(height-offset), self.stride):\n",
    "                    for n in range(offset,(width-offset), self.stride):\n",
    "                        output[batch,depth,m-offset, n-offset]=\\\n",
    "                            np.sum(np.multiply(padded_input[batch,:, m-offset:m+offset+1,n-offset:n+offset+1],\n",
    "                                               self.weights[depth]))\n",
    "        # Output will be of the size (Batch_size, out_channels, out_height, out_width)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_out, learning_rate):\n",
    "        batch_size,channels, height, width = self.input.shape\n",
    "        grad_weights = np.zeros(self.weights.shape)\n",
    "        grad_bias = np.zeros(self.filter)\n",
    "        flip_weights=np.flip(self.weights, axis=(2,3))\n",
    "        grad_input=np.zeros(self.input.shape)\n",
    "        for batch in range(batch_size):\n",
    "            for depth in range(self.filter):\n",
    "                for m in range(0,height - self.filter_size + 1, self.stride):\n",
    "                    for n in range(0, width - self.filter_size + 1, self.stride):\n",
    "                    # grad_W=grad_output* input[kernel_size]\n",
    "                    # grad_W=\n",
    "                        grad_weights[depth] += grad_out[batch, depth, m,n] * self.input[batch,:, m:m+self.filter_size, n:n+self.filter_size]\n",
    "                        grad_bias[depth] += grad_out[batch, depth, m,n]\n",
    "        \n",
    "        if(self.conv_layer!=1):\n",
    "            for batch in range(batch_size):\n",
    "                for m in range(self.out_height):\n",
    "                    for n in range(self.out_width):\n",
    "                        vert_start = m * self.stride\n",
    "                        vert_end = vert_start + self.filter_size\n",
    "                        horiz_start = n * self.stride\n",
    "                        horiz_end = horiz_start + self.filter_size\n",
    "                        for depth in range(self.filter):\n",
    "                            grad_input[batch, :, vert_start:vert_end, horiz_start:horiz_end]+= np.sum(grad_out[batch,depth, m,n] * flip_weights[depth], axis=0)\n",
    "            \n",
    "        \n",
    "        self.weights -= learning_rate * grad_weights / batch\n",
    "        self.bias -= learning_rate * grad_bias / batch\n",
    "\n",
    "        return grad_input\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size, stride,padding=0, dilation=1):\n",
    "        self.pool_size=pool_size\n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "    def forward(self, input_data):\n",
    "        batch_size, in_channels, height, width=input_data.shape\n",
    "        pad_size=self.padding\n",
    "        padded_input=np.zeros(shape=(batch_size, in_channels, height+2*pad_size, width+2*pad_size), dtype='int32')\n",
    "        padded_input[:,:, pad_size:height+pad_size, pad_size: width+pad_size]=input_data\n",
    "        self.input=padded_input\n",
    "        offset=self.pool_size//2\n",
    "        out_height=(height//self.stride) \n",
    "        out_width=(width//self.stride) \n",
    "\n",
    "        output=np.zeros((batch_size, in_channels, out_height, out_width), dtype=\"float64\")\n",
    "        # print(output.shape)\n",
    "        for m in range(0,(height-self.pool_size), self.stride):\n",
    "            for n in range(0,(width-self.pool_size), self.stride):\n",
    "                output[:,: ,m:m+1, n:n+1]= np.max(np.max(padded_input[:,:, m:m+self.pool_size,n:n+self.pool_size],axis=2, keepdims=True ), axis=3, keepdims=True)\n",
    "                \n",
    "        self.max_output=output\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        batch_size, in_channels ,  height, width= self.input.shape\n",
    "        grad_input = np.zeros_like(self.input)\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            for depth in range(in_channels):\n",
    "                for m in range(0, height, self.pool_size):\n",
    "                    for n in range(0, width, self.pool_size):\n",
    "                        window = self.input[batch,depth, m:m+self.pool_size, n:n+self.pool_size]\n",
    "                        max_val = np.max(window)\n",
    "                        grad_input[batch,depth, m:m+self.pool_size, n:n+self.pool_size] = (window == max_val) * grad_out[batch,depth,  m//self.pool_size, n//self.pool_size]\n",
    "        return grad_input\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool2D:\n",
    "    def __init__(self, pool_size, stride,padding=0, dilation=1):\n",
    "        self.pool_size=pool_size\n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "    def forward(self, input_data):\n",
    "        batch_size, in_channels, height, width=input_data.shape\n",
    "        pad_size=self.padding\n",
    "        padded_input=np.zeros(shape=(batch_size, in_channels, height+2*pad_size, width+2*pad_size), dtype='int32')\n",
    "        padded_input[:,:, pad_size:height+pad_size, pad_size: width+pad_size]=input_data\n",
    "        self.input=padded_input\n",
    "        offset=self.pool_size//2\n",
    "        out_height=(height//self.stride) \n",
    "        out_width=(width//self.stride) \n",
    "\n",
    "        output=np.zeros((batch_size, in_channels, out_height, out_width), dtype=\"float64\")\n",
    "        # print(output.shape)\n",
    "        for m in range(0,(height-self.pool_size), self.stride):\n",
    "            for n in range(0,(width-self.pool_size), self.stride):\n",
    "                output[:,: ,m, n]=np.mean(padded_input[:,:, m:m+self.pool_size,n:n+self.pool_size])\n",
    "\n",
    "    def backward(self, grad_loss, lr):\n",
    "        return grad_loss/(self.pool_size)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self=None\n",
    "    def forward(self, input_data):\n",
    "        self.input=input_data\n",
    "        batch=input_data.shape[0]\n",
    "        return input_data.reshape(batch, -1)\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        return grad_out.reshape(self.input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation:\n",
    "  def __init__(self, func):\n",
    "    self.func=func\n",
    "  def forward(self, x):\n",
    "      self.x=x\n",
    "      if(self.func=='sigmoid'):\n",
    "          loss= 1/(1+np.exp(-x))\n",
    "\n",
    "      elif(self.func=='relu'):\n",
    "          loss= np.maximum(0, x)\n",
    "          \n",
    "      elif(self.func=='softmax'):\n",
    "          exp=np.exp(x)\n",
    "          loss=exp/ np.sum(exp, axis=0)\n",
    "\n",
    "      return loss\n",
    "  def backward(self, grad_loss, lr):\n",
    "      if(self.func=='sigmoid'):\n",
    "          loss= 1/(1+np.exp(-self.x))\n",
    "          back_loss= np.exp(-self.x)/ (loss**2)\n",
    "      elif(self.func=='relu'):\n",
    "\n",
    "          back_loss=1*(self.x>0)\n",
    "      elif(self.func=='softmax'):\n",
    "        \n",
    "        t_exp = np.exp(self.x)\n",
    "        # Sum of all e^totals\n",
    "        S = np.sum(t_exp)\n",
    "\n",
    "        back_loss = t_exp * (S - t_exp) / (S ** 2)\n",
    "        \n",
    "      \n",
    "      return grad_loss*back_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotVector(input_y, num_classes):\n",
    "    batch=input_y.shape[0]\n",
    "    one_hot_vector=np.zeros((batch, num_classes))\n",
    "    for b in range(batch):\n",
    "        one_hot_vector[b,int(input_y[b])]=1\n",
    "    return one_hot_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(predicted,input_y):\n",
    "    row,col=input_y.shape\n",
    "    regu=0\n",
    "    epsilon = 1e-12\n",
    "    predicted = np.clip(predicted, epsilon, 1. - epsilon)\n",
    "    y1=np.log(predicted)\n",
    "    \n",
    "    return np.mean(-np.sum(input_y*y1)) , input_y/predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling all the above layers in one CNN Model\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def forward(self, input_data):\n",
    "        output=input_data\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(output)\n",
    "            print(output.shape)\n",
    "        return output\n",
    "    def backward(self,grad_out, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out=layer.backward(grad_out, lr)\n",
    "            print(\" gradout\", grad_out.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load KMNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.load(\"kmnist-train-imgs.npz\")[\"arr_0\"]\n",
    "test_data= np.load(\"kmnist-test-imgs.npz\")['arr_0']\n",
    "train_labels= np.load(\"kmnist-train-labels.npz\")['arr_0']\n",
    "test_labels=np.load(\"kmnist-test-labels.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 3, ..., 9, 4, 2], dtype=uint8)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.reshape(train_data.shape[0], 1, train_data.shape[1], train_data.shape[2])\n",
    "test_data=test_data.reshape((test_data.shape[0], 1, test_data.shape[1], test_data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data in training and validation\n",
    "train_X, validation_X=train_data[: 1000], train_data[1000:]\n",
    "train_y, validation_y=train_labels[: 1000], train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=np.ones((100, 64, 13,13), dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train_y=oneHotVector(train_y, 10)\n",
    "binary_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model1=Model()\n",
    "cnn_model1.add(Conv2D(1, 5, 64))# 1000 x 64 x 24x 24\n",
    "cnn_model1.add(MaxPool2D(2, 2)) # 1000 x64 x12x12\n",
    "cnn_model1.add(activation('sigmoid'))\n",
    "cnn_model1.add(Flatten())#1000x (64. 13.13)  = 1000x 1600\n",
    "cnn_model1.add(Linear((64*12*12), 1000))\n",
    "cnn_model1.add(Linear(1000, 10))\n",
    "cnn_model1.add(activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs=10\n",
    "lr=100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "accuracy=[]\n",
    "for epoch in tqdm(range(epochs), total=epochs):\n",
    "    total_samples=0\n",
    "    total_correct=0\n",
    "    output=cnn_model1.forward(train_X)\n",
    "    # print(output)\n",
    "    y=oneHotVector(train_y, 10)\n",
    "    loss, grad_out= crossEntropy(output, y)\n",
    "    cnn_model1.backward(grad_out, lr)\n",
    "    predicted = np.argmax(output,axis=1)\n",
    "    # print(predicted)\n",
    "    total_samples += train_y.shape[0]\n",
    "    total_correct += (predicted == train_y).sum().item()\n",
    "    acc=total_correct/total_samples\n",
    "    train_loss.append(loss)\n",
    "    accuracy.append(acc)\n",
    "    print(\"Epoch {}: loss= {}, accuracy= {}\".format(epoch, loss/total_samples, acc*100))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
