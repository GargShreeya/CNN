{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        self.weights=np.random.normal(0, 0.1,( input_size, output_size))\n",
    "        self.bias=0\n",
    "        self.in_size=input_size\n",
    "        self.out_size=output_size\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        self.input=input_x\n",
    "        output=np.dot(input_x, self.weights) + self.bias\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_loss, lr):\n",
    "        grad_W=np.dot(self.input.T, grad_loss)\n",
    "        grad_Bias=np.sum(grad_loss)\n",
    "        self.weights-=lr*grad_W/self.input.shape[0]\n",
    "        self.bias-=lr*grad_Bias/ self.input.shape[0]\n",
    "        return (np.dot(grad_loss,self.weights.T))\n",
    "\n",
    "    def backwardL2reg(self, grad_loss, lr):\n",
    "        grad_reg=np.dot(self.weight, grad_loss)\n",
    "        grad_W=np.dot(self.input.T, grad_loss) + grad_reg\n",
    "        grad_Bias=np.sum(grad_loss)\n",
    "        self.weights-=lr*grad_W/self.input.shape[0]\n",
    "        self.bias-=lr*grad_Bias/ self.input.shape[0]\n",
    "        return (np.dot(grad_loss,self.weights.T))\n",
    "\n",
    "    def backwardL1reg(self, grad_loss, lr):\n",
    "        grad_reg=grad_loss\n",
    "        grad_W=np.dot(self.input.T, grad_loss) + grad_reg\n",
    "        grad_Bias=np.sum(grad_loss)\n",
    "        self.weights-=lr*grad_W/self.input.shape[0]\n",
    "        self.bias-=lr*grad_Bias/ self.input.shape[0]\n",
    "        return (np.dot(grad_loss,self.weights.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, in_channels,filter_size,filter_num, padding=0, stride=1, bias=True,  conv_layer=1):\n",
    "        sigma=0.1\n",
    "        self.filter_size=filter_size\n",
    "        self.filter=filter_num\n",
    "        self.padding=padding\n",
    "        self.stride=stride\n",
    "        self.in_channels=in_channels\n",
    "        self.weights=np.random.normal(0, sigma, size=(filter_num,in_channels,filter_size, filter_size))\n",
    "        # self.weights=np.random.randn(filter_num,in_channels,filter_size, filter_size)\n",
    "        self.bias=np.zeros(self.filter)\n",
    "        self.conv_layer=conv_layer\n",
    "        \n",
    "    def forward(self,input):\n",
    "        batch_size, in_channels, height, width=input.shape\n",
    "        #Input shape= (Batch_size, in_channels, in_height, in_width)\n",
    "        pad_size=self.padding\n",
    "        #adding padd to the input\n",
    "        padded_input=np.zeros(shape=(batch_size, in_channels, height+2*pad_size, width+2*pad_size), dtype='int32')\n",
    "        padded_input[:,:, pad_size:height+pad_size, pad_size: width+pad_size]=input\n",
    "        # Reminder: Save Input for backward-prop\n",
    "        self.input=padded_input\n",
    "        # Simple Conv operation:\n",
    "        self.out_height=((height-self.filter_size + 2*pad_size)//self.stride)+1\n",
    "        self.out_width=((width-self.filter_size + 2*pad_size)//self.stride)+1\n",
    "        self.out_channel=self.filter\n",
    "        output=np.zeros(shape=(batch_size, self.out_channel,self.out_height, self.out_width ), dtype='float64')\n",
    "        # Loop over every location in inp_height * inp_width for the whole batch\n",
    "        offset=self.filter_size//2\n",
    "        for depth in range(self.out_channel):\n",
    "            for batch in range(batch_size):\n",
    "                for m in range(offset,(height-offset), self.stride):\n",
    "                    for n in range(offset,(width-offset), self.stride):\n",
    "                        output[batch,depth,m-offset, n-offset]=\\\n",
    "                            np.sum(np.multiply(padded_input[batch,:, m-offset:m+offset+1,n-offset:n+offset+1],\n",
    "                                               self.weights[depth]))\n",
    "        # Output will be of the size (Batch_size, out_channels, out_height, out_width)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_out, learning_rate):\n",
    "        batch_size,channels, height, width = self.input.shape\n",
    "        grad_weights = np.zeros(self.weights.shape)\n",
    "        grad_bias = np.zeros(self.filter)\n",
    "        flip_weights=np.flip(self.weights, axis=(2,3))\n",
    "        grad_input=np.zeros(self.input.shape)\n",
    "        for batch in range(batch_size):\n",
    "            for depth in range(self.filter):\n",
    "                for m in range(0,height - self.filter_size + 1, self.stride):\n",
    "                    for n in range(0, width - self.filter_size + 1, self.stride):\n",
    "                    # grad_W=grad_output* input[kernel_size]\n",
    "                    # grad_W=\n",
    "                        grad_weights[depth] += grad_out[batch, depth, m,n] * self.input[batch,:, m:m+self.filter_size, n:n+self.filter_size]\n",
    "                        grad_bias[depth] += grad_out[batch, depth, m,n]\n",
    "        \n",
    "        if(self.conv_layer!=1):\n",
    "            for batch in range(batch_size):\n",
    "                for m in range(self.out_height):\n",
    "                    for n in range(self.out_width):\n",
    "                        vert_start = m * self.stride\n",
    "                        vert_end = vert_start + self.filter_size\n",
    "                        horiz_start = n * self.stride\n",
    "                        horiz_end = horiz_start + self.filter_size\n",
    "                        for depth in range(self.filter):\n",
    "                            grad_input[batch, :, vert_start:vert_end, horiz_start:horiz_end]+= np.sum(grad_out[batch,depth, m,n] * flip_weights[depth], axis=0)\n",
    "            \n",
    "        \n",
    "        self.weights -= learning_rate * grad_weights / batch\n",
    "        self.bias -= learning_rate * grad_bias / batch\n",
    "\n",
    "        return grad_input\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size, stride,padding=0, dilation=1):\n",
    "        self.pool_size=pool_size\n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "    def forward(self, input_data):\n",
    "        batch_size, in_channels, height, width=input_data.shape\n",
    "        pad_size=self.padding\n",
    "        padded_input=np.zeros(shape=(batch_size, in_channels, height+2*pad_size, width+2*pad_size), dtype='int32')\n",
    "        padded_input[:,:, pad_size:height+pad_size, pad_size: width+pad_size]=input_data\n",
    "        self.input=padded_input\n",
    "        offset=self.pool_size//2\n",
    "        out_height=(height//self.stride) \n",
    "        out_width=(width//self.stride) \n",
    "\n",
    "        output=np.zeros((batch_size, in_channels, out_height, out_width), dtype=\"float64\")\n",
    "        # print(output.shape)\n",
    "        for m in range(0,(height-self.pool_size), self.stride):\n",
    "            for n in range(0,(width-self.pool_size), self.stride):\n",
    "                output[:,: ,m:m+1, n:n+1]= np.max(np.max(padded_input[:,:, m:m+self.pool_size,n:n+self.pool_size],axis=2, keepdims=True ), axis=3, keepdims=True)\n",
    "                \n",
    "        self.max_output=output\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        batch_size, in_channels ,  height, width= self.input.shape\n",
    "        grad_input = np.zeros_like(self.input)\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            for depth in range(in_channels):\n",
    "                for m in range(0, height, self.pool_size):\n",
    "                    for n in range(0, width, self.pool_size):\n",
    "                        window = self.input[batch,depth, m:m+self.pool_size, n:n+self.pool_size]\n",
    "                        max_val = np.max(window)\n",
    "                        grad_input[batch,depth, m:m+self.pool_size, n:n+self.pool_size] = (window == max_val) * grad_out[batch,depth,  m//self.pool_size, n//self.pool_size]\n",
    "        return grad_input\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool2D:\n",
    "    def __init__(self, pool_size, stride,padding=0, dilation=1):\n",
    "        self.pool_size=pool_size\n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "    def forward(self, input_data):\n",
    "        batch_size, in_channels, height, width=input_data.shape\n",
    "        pad_size=self.padding\n",
    "        padded_input=np.zeros(shape=(batch_size, in_channels, height+2*pad_size, width+2*pad_size), dtype='int32')\n",
    "        padded_input[:,:, pad_size:height+pad_size, pad_size: width+pad_size]=input_data\n",
    "        self.input=padded_input\n",
    "        offset=self.pool_size//2\n",
    "        out_height=(height//self.stride) \n",
    "        out_width=(width//self.stride) \n",
    "\n",
    "        output=np.zeros((batch_size, in_channels, out_height, out_width), dtype=\"float64\")\n",
    "        # print(output.shape)\n",
    "        for m in range(0,(height-self.pool_size), self.stride):\n",
    "            for n in range(0,(width-self.pool_size), self.stride):\n",
    "                output[:,: ,m, n]=np.mean(padded_input[:,:, m:m+self.pool_size,n:n+self.pool_size])\n",
    "\n",
    "    def backward(self, grad_loss, lr):\n",
    "        return grad_loss/(self.pool_size)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self=None\n",
    "    def forward(self, input_data):\n",
    "        self.input=input_data\n",
    "        batch=input_data.shape[0]\n",
    "        return input_data.reshape(batch, -1)\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        return grad_out.reshape(self.input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation:\n",
    "  def __init__(self, func):\n",
    "    self.func=func\n",
    "  def forward(self, x):\n",
    "      self.x=x\n",
    "      if(self.func=='sigmoid'):\n",
    "          loss= 1/(1+np.exp(-x))\n",
    "\n",
    "      elif(self.func=='relu'):\n",
    "          loss= np.maximum(0, x)\n",
    "          \n",
    "      elif(self.func=='softmax'):\n",
    "          exp=np.exp(x)\n",
    "          loss=exp/ np.sum(exp, axis=0)\n",
    "\n",
    "      return loss\n",
    "  def backward(self, grad_loss, lr):\n",
    "      if(self.func=='sigmoid'):\n",
    "          loss= 1/(1+np.exp(-self.x))\n",
    "          back_loss= np.exp(-self.x)/ (loss**2)\n",
    "      elif(self.func=='relu'):\n",
    "\n",
    "          back_loss=1*(self.x>0)\n",
    "      elif(self.func=='softmax'):\n",
    "        \n",
    "        t_exp = np.exp(self.x)\n",
    "        # Sum of all e^totals\n",
    "        S = np.sum(t_exp)\n",
    "\n",
    "        back_loss = t_exp * (S - t_exp) / (S ** 2)\n",
    "        \n",
    "      \n",
    "      return grad_loss*back_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotVector(input_y, num_classes):\n",
    "    batch=input_y.shape[0]\n",
    "    one_hot_vector=np.zeros((batch, num_classes))\n",
    "    for b in range(batch):\n",
    "        one_hot_vector[b,int(input_y[b])]=1\n",
    "    return one_hot_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(predicted,input_y):\n",
    "    row,col=input_y.shape\n",
    "    regu=0\n",
    "    epsilon = 1e-12\n",
    "    predicted = np.clip(predicted, epsilon, 1. - epsilon)\n",
    "    y1=np.log(predicted)\n",
    "    \n",
    "    return np.mean(-np.sum(input_y*y1)) , input_y/predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling all the above layers in one CNN Model\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def forward(self, input_data):\n",
    "        output=input_data\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(output)\n",
    "            print(output.shape)\n",
    "        return output\n",
    "    def backward(self,grad_out, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out=layer.backward(grad_out, lr)\n",
    "            print(\" gradout\", grad_out.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load KMNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.load(\"kmnist-train-imgs.npz\")[\"arr_0\"]\n",
    "test_data= np.load(\"kmnist-test-imgs.npz\")['arr_0']\n",
    "train_labels= np.load(\"kmnist-train-labels.npz\")['arr_0']\n",
    "test_labels=np.load(\"kmnist-test-labels.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 3, ..., 9, 4, 2], dtype=uint8)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.reshape(train_data.shape[0], 1, train_data.shape[1], train_data.shape[2])\n",
    "test_data=test_data.reshape((test_data.shape[0], 1, test_data.shape[1], test_data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.3254902 , 0.57647059, 0.49411765, ..., 0.56862745,\n",
       "          0.19215686, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.00392157, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.00784314, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        , ..., 0.00392157,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.51372549, ..., 0.31764706,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.44705882, ..., 0.09019608,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.04313725, ..., 0.        ,\n",
       "          0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=train_data/255\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data in training and validation\n",
    "train_X, validation_X=train_data[: 1000], train_data[1000:]\n",
    "train_y, validation_y=train_labels[: 1000], train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 7, 0, 1, 4, 2, 4, 8, 1, 1, 5, 1, 0, 5, 7, 6, 1, 7, 9, 5, 7, 3,\n",
       "       7, 5, 6, 6, 2, 7, 6, 0, 9, 6, 1, 5, 9, 5, 8, 0, 0, 8, 8, 6, 7, 7,\n",
       "       7, 8, 1, 9, 6, 0, 5, 1, 1, 1, 3, 2, 2, 6, 4, 3, 5, 5, 4, 6, 6, 1,\n",
       "       7, 8, 8, 3, 1, 9, 9, 3, 2, 1, 0, 4, 8, 2, 3, 6, 9, 9, 6, 5, 6, 1,\n",
       "       0, 7, 2, 2, 8, 0, 1, 8, 6, 6, 3, 5, 0, 9, 6, 9, 8, 2, 1, 3, 9, 4,\n",
       "       6, 5, 3, 4, 7, 0, 4, 8, 6, 5, 5, 4, 2, 2, 2, 5, 6, 6, 8, 1, 5, 5,\n",
       "       6, 2, 9, 2, 0, 5, 5, 2, 7, 5, 6, 8, 8, 0, 3, 4, 5, 6, 3, 5, 0, 1,\n",
       "       4, 4, 7, 7, 7, 8, 6, 6, 5, 2, 7, 5, 2, 1, 8, 1, 5, 5, 3, 6, 1, 0,\n",
       "       1, 0, 3, 7, 0, 9, 9, 8, 0, 0, 0, 9, 5, 9, 6, 5, 4, 5, 3, 8, 8, 3,\n",
       "       6, 0, 8, 4, 5, 7, 0, 8, 4, 5, 1, 1, 2, 3, 8, 7, 3, 7, 4, 7, 2, 9,\n",
       "       1, 7, 9, 5, 7, 4, 6, 9, 0, 1, 0, 1, 2, 6, 5, 9, 0, 5, 7, 0, 6, 8,\n",
       "       2, 1, 9, 7, 7, 9, 8, 6, 6, 1, 9, 1, 0, 8, 5, 2, 7, 1, 4, 5, 0, 5,\n",
       "       4, 7, 1, 2, 3, 7, 0, 8, 5, 9, 5, 2, 3, 2, 3, 2, 7, 3, 4, 0, 9, 5,\n",
       "       0, 3, 5, 4, 6, 3, 8, 6, 7, 4, 7, 1, 8, 4, 1, 8, 8, 3, 9, 8, 6, 8,\n",
       "       0, 0, 7, 4, 7, 1, 2, 4, 4, 2, 4, 4, 6, 4, 5, 9, 3, 4, 2, 4, 1, 5,\n",
       "       5, 2, 2, 9, 3, 4, 4, 9, 7, 8, 8, 1, 8, 9, 9, 3, 5, 6, 2, 4, 6, 8,\n",
       "       0, 9, 6, 3, 8, 3, 3, 4, 4, 4, 7, 7, 2, 5, 3, 6, 5, 3, 4, 1, 2, 9,\n",
       "       9, 5, 4, 8, 3, 6, 3, 2, 5, 8, 3, 6, 9, 0, 5, 8, 6, 1, 5, 0, 8, 0,\n",
       "       3, 5, 3, 9, 9, 2, 0, 5, 2, 4, 8, 9, 9, 9, 3, 7, 4, 6, 5, 3, 1, 6,\n",
       "       8, 0, 4, 3, 6, 1, 0, 7, 7, 2, 2, 3, 3, 2, 5, 8, 0, 6, 7, 7, 4, 8,\n",
       "       6, 0, 4, 4, 8, 6, 7, 9, 0, 2, 0, 8, 5, 2, 6, 8, 9, 1, 2, 2, 9, 2,\n",
       "       6, 2, 7, 1, 9, 9, 8, 9, 4, 7, 0, 3, 5, 6, 0, 2, 7, 1, 3, 2, 8, 5,\n",
       "       3, 2, 0, 8, 6, 5, 0, 7, 6, 5, 3, 4, 5, 5, 2, 1, 1, 5, 3, 2, 2, 2,\n",
       "       3, 6, 0, 9, 2, 8, 8, 5, 1, 8, 4, 5, 1, 0, 7, 1, 8, 1, 6, 9, 7, 3,\n",
       "       1, 3, 9, 0, 8, 8, 9, 3, 7, 4, 4, 5, 9, 5, 6, 7, 7, 4, 9, 6, 4, 5,\n",
       "       7, 4, 7, 3, 7, 7, 2, 3, 7, 7, 0, 5, 2, 3, 7, 8, 9, 7, 5, 9, 9, 0,\n",
       "       6, 4, 3, 3, 8, 9, 6, 3, 7, 1, 8, 2, 5, 6, 9, 3, 2, 8, 1, 2, 7, 8,\n",
       "       6, 3, 3, 4, 5, 5, 8, 7, 2, 6, 9, 2, 1, 7, 9, 4, 1, 0, 2, 5, 2, 9,\n",
       "       5, 7, 3, 4, 2, 1, 3, 4, 0, 3, 9, 3, 4, 4, 6, 0, 3, 1, 7, 5, 2, 3,\n",
       "       8, 9, 5, 0, 5, 9, 9, 4, 5, 2, 1, 2, 2, 0, 2, 4, 2, 5, 4, 8, 1, 6,\n",
       "       6, 7, 1, 3, 3, 1, 3, 0, 5, 9, 5, 3, 7, 6, 0, 8, 7, 5, 7, 1, 3, 4,\n",
       "       6, 8, 8, 1, 2, 2, 1, 5, 4, 8, 9, 4, 2, 4, 1, 1, 9, 3, 1, 9, 9, 3,\n",
       "       7, 0, 3, 3, 3, 6, 4, 1, 5, 2, 8, 7, 5, 8, 3, 9, 9, 8, 7, 6, 5, 9,\n",
       "       7, 3, 9, 4, 1, 7, 0, 6, 1, 4, 5, 0, 9, 4, 6, 2, 6, 7, 0, 3, 8, 2,\n",
       "       3, 1, 0, 0, 3, 7, 0, 4, 9, 1, 2, 5, 1, 5, 8, 8, 9, 3, 8, 2, 7, 3,\n",
       "       4, 6, 1, 9, 5, 4, 4, 9, 5, 9, 5, 9, 8, 2, 9, 1, 2, 0, 6, 8, 1, 8,\n",
       "       3, 0, 6, 8, 0, 4, 3, 4, 2, 9, 2, 2, 6, 0, 6, 8, 2, 2, 7, 2, 6, 4,\n",
       "       4, 9, 4, 8, 2, 8, 8, 9, 2, 4, 4, 6, 7, 1, 0, 1, 5, 2, 1, 8, 2, 9,\n",
       "       9, 9, 2, 9, 6, 2, 5, 3, 8, 9, 4, 7, 7, 2, 3, 7, 6, 1, 9, 6, 0, 2,\n",
       "       0, 8, 9, 5, 5, 2, 1, 2, 9, 7, 0, 7, 5, 8, 5, 3, 8, 9, 5, 8, 5, 9,\n",
       "       3, 9, 1, 8, 4, 7, 9, 5, 3, 5, 3, 0, 2, 6, 3, 4, 5, 2, 5, 2, 6, 0,\n",
       "       6, 6, 8, 5, 9, 9, 4, 1, 1, 3, 9, 2, 0, 4, 1, 4, 0, 9, 5, 7, 1, 4,\n",
       "       3, 8, 4, 2, 3, 3, 6, 5, 5, 4, 3, 7, 9, 2, 3, 0, 9, 9, 0, 6, 9, 4,\n",
       "       4, 1, 9, 0, 3, 9, 6, 7, 9, 7, 1, 2, 9, 9, 5, 6, 2, 9, 0, 3, 4, 2,\n",
       "       1, 0, 6, 0, 1, 9, 9, 2, 8, 0, 4, 8, 5, 2, 7, 7, 4, 8, 7, 4, 8, 1,\n",
       "       1, 2, 4, 3, 3, 8, 7, 8, 0, 7], dtype=uint8)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=np.ones((100, 64, 13,13), dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64, 1, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,: , 0:1,0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train_y=oneHotVector(train_y, 10)\n",
    "binary_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model1=Model()\n",
    "cnn_model1.add(Conv2D(1, 5, 64))# 1000 x 64 x 24x 24\n",
    "cnn_model1.add(MaxPool2D(2, 2)) # 1000 x64 x12x12\n",
    "cnn_model1.add(activation('sigmoid'))\n",
    "cnn_model1.add(Flatten())#1000x (64. 13.13)  = 1000x 1600\n",
    "cnn_model1.add(Linear((64*12*12), 1000))\n",
    "cnn_model1.add(Linear(1000, 10))\n",
    "cnn_model1.add(activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs=10\n",
    "lr=100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ef6087bee34de090084202a64c3375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss= 6.903957331009915, accuracy= 12.5\n",
      "Epoch 1: loss= 6.903950705600238, accuracy= 12.5\n",
      "Epoch 2: loss= 6.903943962828895, accuracy= 12.5\n",
      "Epoch 3: loss= 6.903937283077984, accuracy= 12.5\n",
      "Epoch 4: loss= 6.903930671391561, accuracy= 12.5\n",
      "Epoch 5: loss= 6.90392412820671, accuracy= 12.5\n",
      "Epoch 6: loss= 6.90391765389397, accuracy= 12.4\n",
      "Epoch 7: loss= 6.903911248846949, accuracy= 12.4\n",
      "Epoch 8: loss= 6.903904913484495, accuracy= 12.4\n",
      "Epoch 9: loss= 6.903898648251147, accuracy= 12.4\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "accuracy=[]\n",
    "for epoch in tqdm(range(epochs), total=epochs):\n",
    "    total_samples=0\n",
    "    total_correct=0\n",
    "    output=cnn_model1.forward(train_X)\n",
    "    # print(output)\n",
    "    y=oneHotVector(train_y, 10)\n",
    "    loss, grad_out= crossEntropy(output, y)\n",
    "    cnn_model1.backward(grad_out, lr)\n",
    "    predicted = np.argmax(output,axis=1)\n",
    "    # print(predicted)\n",
    "    total_samples += train_y.shape[0]\n",
    "    total_correct += (predicted == train_y).sum().item()\n",
    "    acc=total_correct/total_samples\n",
    "    train_loss.append(loss)\n",
    "    accuracy.append(acc)\n",
    "    print(\"Epoch {}: loss= {}, accuracy= {}\".format(epoch, loss/total_samples, acc*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model2=Model()\n",
    "cnn_model2.add(Conv2D(1,5, 128, conv_layer=1))  # 1000 x 128 x 24x 24 \n",
    "cnn_model2.add(MaxPool2D(2,2)) # 1000x 128 x 12 x12\n",
    "cnn_model2.add(activation('sigmoid'))\n",
    "cnn_model2.add(Conv2D(128, 3, 64, conv_layer=2))# 1000 x 64 x 10x 10\n",
    "cnn_model2.add(MaxPool2D(2,2)) # 1000x 64x 5x5\n",
    "cnn_model2.add(activation('sigmoid'))\n",
    "cnn_model2.add(Flatten())#1000x (64. 5.5)  \n",
    "cnn_model2.add(Linear((64*5*5), 100))\n",
    "cnn_model2.add(Linear(100, 10))\n",
    "cnn_model2.add(activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56683e1c2d9148c0a6c13caf8a9e37ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 128, 24, 24)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 64, 10, 10)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 1600)\n",
      "(1000, 100)\n",
      "(1000, 10)\n",
      "(1000, 10)\n",
      " gradout (1000, 10)\n",
      " gradout (1000, 100)\n",
      " gradout (1000, 1600)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 10, 10)\n",
      " gradout (1000, 128, 12, 12)\n",
      " gradout (1000, 128, 12, 12)\n",
      " gradout (1000, 128, 24, 24)\n",
      " gradout (1000, 1, 28, 28)\n",
      "Epoch 0: loss= 6.907755278982138, accuracy= 10.100000000000001\n",
      "(1000, 128, 24, 24)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 64, 10, 10)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 1600)\n",
      "(1000, 100)\n",
      "(1000, 10)\n",
      "(1000, 10)\n",
      " gradout (1000, 10)\n",
      " gradout (1000, 100)\n",
      " gradout (1000, 1600)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 5, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2590467/3492444217.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  exp=np.exp(x)\n",
      "/tmp/ipykernel_2590467/3492444217.py:14: RuntimeWarning: invalid value encountered in divide\n",
      "  loss=exp/ np.sum(exp, axis=0)\n",
      "/tmp/ipykernel_2590467/3492444217.py:26: RuntimeWarning: overflow encountered in exp\n",
      "  t_exp = np.exp(self.x)\n",
      "/tmp/ipykernel_2590467/3492444217.py:30: RuntimeWarning: invalid value encountered in divide\n",
      "  back_loss = -t_exp * t_exp / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:31: RuntimeWarning: invalid value encountered in subtract\n",
      "  back_loss = t_exp * (S - t_exp) / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:31: RuntimeWarning: invalid value encountered in multiply\n",
      "  back_loss = t_exp * (S - t_exp) / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  back_loss = t_exp * (S - t_exp) / (S ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradout (1000, 64, 10, 10)\n",
      " gradout (1000, 128, 12, 12)\n",
      " gradout (1000, 128, 12, 12)\n",
      " gradout (1000, 128, 24, 24)\n",
      " gradout (1000, 1, 28, 28)\n",
      "Epoch 1: loss= nan, accuracy= 8.7\n",
      "(1000, 128, 24, 24)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 128, 12, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2590467/3492444217.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  loss= 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 10, 10)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 1600)\n",
      "(1000, 100)\n",
      "(1000, 10)\n",
      "(1000, 10)\n",
      " gradout (1000, 10)\n",
      " gradout (1000, 100)\n",
      " gradout (1000, 1600)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 10, 10)\n",
      " gradout (1000, 128, 12, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2590467/3492444217.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  loss= 1/(1+np.exp(-self.x))\n",
      "/tmp/ipykernel_2590467/3492444217.py:20: RuntimeWarning: overflow encountered in exp\n",
      "  back_loss= np.exp(-self.x)/ (loss**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradout (1000, 128, 12, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2590467/4239141351.py:35: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad_input[batch,depth, m:m+self.pool_size, n:n+self.pool_size] = (window == max_val) * grad_out[batch,depth,  m//self.pool_size, n//self.pool_size]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradout (1000, 128, 24, 24)\n",
      " gradout (1000, 1, 28, 28)\n",
      "Epoch 2: loss= nan, accuracy= 8.7\n",
      "(1000, 128, 24, 24)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 128, 12, 12)\n",
      "(1000, 64, 10, 10)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 64, 5, 5)\n",
      "(1000, 1600)\n",
      "(1000, 100)\n",
      "(1000, 10)\n",
      "(1000, 10)\n",
      " gradout (1000, 10)\n",
      " gradout (1000, 100)\n",
      " gradout (1000, 1600)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 5, 5)\n",
      " gradout (1000, 64, 10, 10)\n",
      " gradout (1000, 128, 12, 12)\n",
      " gradout (1000, 128, 12, 12)\n",
      " gradout (1000, 128, 24, 24)\n",
      " gradout (1000, 1, 28, 28)\n",
      "Epoch 3: loss= nan, accuracy= 8.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m total_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m total_correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m output\u001b[38;5;241m=\u001b[39mcnn_model2\u001b[38;5;241m.\u001b[39mforward(train_X)\n\u001b[1;32m     10\u001b[0m y\u001b[38;5;241m=\u001b[39moneHotVector(train_y, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m loss, grad_out\u001b[38;5;241m=\u001b[39m crossEntropy(output, y)\n",
      "Cell \u001b[0;32mIn[239], line 10\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m output\u001b[38;5;241m=\u001b[39minput_data\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 10\u001b[0m     output\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mforward(output)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[265], line 35\u001b[0m, in \u001b[0;36mConv2D.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(offset,(height\u001b[38;5;241m-\u001b[39moffset), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride):\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(offset,(width\u001b[38;5;241m-\u001b[39moffset), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride):\n\u001b[1;32m     34\u001b[0m                 output[batch,depth,m\u001b[38;5;241m-\u001b[39moffset, n\u001b[38;5;241m-\u001b[39moffset]\u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m---> 35\u001b[0m                     np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mmultiply(padded_input[batch,:, m\u001b[38;5;241m-\u001b[39moffset:m\u001b[38;5;241m+\u001b[39moffset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n\u001b[38;5;241m-\u001b[39moffset:n\u001b[38;5;241m+\u001b[39moffset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     36\u001b[0m                                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[depth]))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Output will be of the size (Batch_size, out_channels, out_height, out_width)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs =12\n",
    "lr=1000\n",
    "train_loss=[]\n",
    "accuracy=[]\n",
    "for epoch in tqdm(range(epochs), total=epochs):\n",
    "    total_samples=0\n",
    "    total_correct=0\n",
    "    output=cnn_model2.forward(train_X)\n",
    "    \n",
    "    y=oneHotVector(train_y, 10)\n",
    "    loss, grad_out= crossEntropy(output, y)\n",
    "    cnn_model2.backward(grad_out, lr)\n",
    "    predicted = np.argmax(output,axis=1)\n",
    "    total_samples += train_y.shape[0]\n",
    "    total_correct += (predicted == train_y).sum().item()\n",
    "    acc=total_correct/total_samples\n",
    "    train_loss.append(loss)\n",
    "    accuracy.append(acc)\n",
    "    print(\"Epoch {}: loss= {}, accuracy= {}\".format(epoch, loss/total_samples, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "binary_data=np.loadtxt(\"/data/home1/shreeyagarg/AIP/Assignment4/binary_classification_data_group_6_test.txt\", delimiter=\"\\t\", skiprows=1)\n",
    "idx=random.sample(range(binary_data.shape[0]), 1000)\n",
    "binarydata=binary_data[:, :10]\n",
    "labels=binary_data[:, 10]\n",
    "training_data=binarydata[idx]\n",
    "training_labels=labels[idx]\n",
    "res=[ele for i, ele in enumerate(list(range(binary_data.shape[0]))) if i not in idx]\n",
    "testing_data=binary_data[res]\n",
    "testing_labels=labels[res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.reshape(training_data.shape[0], 1, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model3=Model()\n",
    "cnn_model3.add(Conv2D(1,1, 128, conv_layer=1))  # 1000 x 128 x 10x1 \n",
    "# cnn_model3.add(activation('relu'))\n",
    "cnn_model3.add(Conv2D(128, 1, 64, conv_layer=2))# 1000 x 64 x 10x 1\n",
    "# cnn_model3.add(activation('relu'))\n",
    "cnn_model3.add(Flatten())#1000x (64. 10.1)  \n",
    "cnn_model3.add(Linear((640), 2))\n",
    "# cnn_model3.add(Linear(100, 2))\n",
    "cnn_model3.add(activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aa2bc5ea96489a8b90a51827be6389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 128, 10, 1)\n",
      "(1000, 64, 10, 1)\n",
      "(1000, 640)\n",
      "(1000, 2)\n",
      "(1000, 2)\n",
      " gradout (1000, 2)\n",
      " gradout (1000, 640)\n",
      " gradout (1000, 64, 10, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2590467/3492444217.py:30: RuntimeWarning: overflow encountered in multiply\n",
      "  back_loss = -t_exp * t_exp / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:30: RuntimeWarning: overflow encountered in double_scalars\n",
      "  back_loss = -t_exp * t_exp / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:30: RuntimeWarning: invalid value encountered in divide\n",
      "  back_loss = -t_exp * t_exp / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:31: RuntimeWarning: overflow encountered in multiply\n",
      "  back_loss = t_exp * (S - t_exp) / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "  back_loss = t_exp * (S - t_exp) / (S ** 2)\n",
      "/tmp/ipykernel_2590467/3492444217.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  back_loss = t_exp * (S - t_exp) / (S ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradout (1000, 128, 10, 1)\n",
      " gradout (1000, 1, 10, 1)\n",
      "Epoch 0: loss= 27.603390094812656, accuracy= 8.5\n",
      "(1000, 128, 10, 1)\n",
      "(1000, 64, 10, 1)\n",
      "(1000, 640)\n",
      "(1000, 2)\n",
      "(1000, 2)\n",
      " gradout (1000, 2)\n",
      " gradout (1000, 640)\n",
      " gradout (1000, 64, 10, 1)\n",
      " gradout (1000, 128, 10, 1)\n",
      " gradout (1000, 1, 10, 1)\n",
      "Epoch 1: loss= nan, accuracy= 8.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[293], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m total_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m total_correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m output\u001b[38;5;241m=\u001b[39mcnn_model3\u001b[38;5;241m.\u001b[39mforward(training_data)\n\u001b[1;32m     10\u001b[0m y\u001b[38;5;241m=\u001b[39moneHotVector(training_labels, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m loss, grad_out\u001b[38;5;241m=\u001b[39m crossEntropy(output, y)\n",
      "Cell \u001b[0;32mIn[239], line 10\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m output\u001b[38;5;241m=\u001b[39minput_data\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 10\u001b[0m     output\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mforward(output)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[265], line 35\u001b[0m, in \u001b[0;36mConv2D.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(offset,(height\u001b[38;5;241m-\u001b[39moffset), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride):\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(offset,(width\u001b[38;5;241m-\u001b[39moffset), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride):\n\u001b[1;32m     34\u001b[0m                 output[batch,depth,m\u001b[38;5;241m-\u001b[39moffset, n\u001b[38;5;241m-\u001b[39moffset]\u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m---> 35\u001b[0m                     np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mmultiply(padded_input[batch,:, m\u001b[38;5;241m-\u001b[39moffset:m\u001b[38;5;241m+\u001b[39moffset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n\u001b[38;5;241m-\u001b[39moffset:n\u001b[38;5;241m+\u001b[39moffset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     36\u001b[0m                                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[depth]))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Output will be of the size (Batch_size, out_channels, out_height, out_width)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:69\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     70\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs =20\n",
    "lr=0.001\n",
    "train_loss=[]\n",
    "accuracy=[]\n",
    "for epoch in tqdm(range(epochs), total=epochs):\n",
    "    total_samples=0\n",
    "    total_correct=0\n",
    "    output=cnn_model3.forward(training_data)\n",
    "    \n",
    "    y=oneHotVector(training_labels, 2)\n",
    "    loss, grad_out= crossEntropy(output, y)\n",
    "    cnn_model3.backward(grad_out, lr)\n",
    "    predicted = np.argmax(output,axis=1)\n",
    "    total_samples += train_y.shape[0]\n",
    "    total_correct += (predicted == train_y).sum().item()\n",
    "    acc=total_correct/total_samples\n",
    "    train_loss.append(loss)\n",
    "    accuracy.append(acc)\n",
    "    print(\"Epoch {}: loss= {}, accuracy= {}\".format(epoch, loss/total_samples, acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
